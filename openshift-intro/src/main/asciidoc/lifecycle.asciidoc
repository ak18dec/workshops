[[lifecycle]]
== Simple application lifecycle

****
IMPORTANT: *TODO*

Describe the scripts within an S2I build (compile, assemble, execute)

****

[[lifecycle-s2i]]
=== Deploying an application using S2I

In the Catalog of your sandbox project:

* Filter (top tab bar) *Languages*
* Select *JavaScript*
* Choose *Node.js*

image::screenshot_catalog_filter_js.png[Node application]

You will now go though a wizzard to gather all data needed for this S2I build config:

****
IMPORTANT: *TODO*

Explain the nature of the wizards

****

* Click btn:[Next] on first page of Wizard.

****
IMPORTANT: *TODO*

Explain the parameter fields as pertains to templates

****

* Select node image version *10*
* Enter name as `nodetest`
* Enter the following url as the github repo https://github.com/utherp0/ocpnode[https://github.com/utherp0/ocpnode]

Click btn:[Create], then btn:[Close] to close the wizard

image::screenshot_s2i_wizard.png[S2I Wizard]

Now go back to the menu:Overview[] page.

****
IMPORTANT: *TODO*

Explain the behaviour that is happening, the creation of the objects from the template, 
the build-config being used by the build, the build delivering the image into the registry, 
the deployment-config waiting on the image arrival, the default single Pod deployment, the creation of the route

****
[[lifecycle-running]]
=== The running application

When in the Overview page, you will see all running applications. Expand the `nodetest` application we just deployed.
You will see an overview of the running application:

* Information on the running container
* Number of pods and the status (a.k.a kbd:[Pod ring]) of the pods
* Networking information including internal port mapping and external routes
* Build history and information

image::screenshot_app_overview.png[Application Overview]

To see the application in action, click on the link in the external route.
This will open the basic node.js application:

image::screenshot_node_app.png[Node.js Application]

[[lifecycle-application-services]]
==== Application Services

Using the menu on the left go to the menu:Applications[Services] page.

image::screenshot_app_services.png[Application Services]

****
IMPORTANT: *TODO*

Explain the nature of the single service endpoint for the application - note the cluster IP address

****

NOTE: More info here: 
https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/pods_and_services.html#services[https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/pods_and_services.html#services]

Go back to the menu:Overview[] page.

[[lifecycle-application-pods]]
==== Application Pods

Click on the kbd:[Pod ring], or alternatively use the menu menu:Applications[Pods > nodetest-****]

image::screenshot_app_pod.png[Application Pod]

TIP: See the differing IP address for the Pod compared to the cluster IP

Go back to the menu:Overview[] page.

[[lifecycle-application-scaling]]
==== Application Scaling

Let's pretend that this app is suddenly getting many requests from many users (so there is a load increase on the app).
So we need to scale the application to 3 instances.

Click the menu:Up arrow[] (^) until there are 3 replicas.

image::screenshot_scaling_up.png[Application Scaling]

****
IMPORTANT: *TODO*

Explain the blue, gray, dark blue, red colour schemes for the Pod behaviors

****

Click on the kbd:[Pod ring], or alternatively use the menu menu:Applications[Deployments > nodetest > #1 (latest)].

Scroll down to where the Pods are listed:

image::screenshot_app_pods_3.png[Pod listing]

TIP: See the difference in age between the initial pod and the 2 recent scaled pods.

Select on of the recent (younger) pods.

TIP: Note the IP difference compared to the initial pod.

****
IMPORTANT: *TODO*

Explain the load-balancing of Pod IP endpoints from the singular cluster IP and how that abstracts from the Route.

****

[[lifecycle-application-route]]
==== Application Route

Using the menu on the left go to the menu:Applications[Routes] page.

image::screenshot_app_routes.png[Application Routes]

TIP: Note the mapping of the fully qualified domain name to the cluster IP via the service name

Select the nodetest link in the service column. 

image::screenshot_route_service.png[Route service]

TIP: Note that the route maps to the cluster IP

[[lifecycle-application-cli]]
==== Application from CLI

Now let's go to the console (either using `localhost` or `oconline` as explained in the <<setup-cli>> section)

Make sure you are still logged in:

[source,shell]
----
oc whoami
----

(if not, log in again as explained in the <<setup-login>> section)

Make sure we are using our sandbox project:

[source,shell]
----
oc project sandbox-userX
----

This will print: 

[source,shell,subs=attributes+]
----
Now using project "sandbox-userX" on server "{webConsoleUrl}:443".
----

You can find all `objects` that you can interact with in this namespace/project:

[source,shell]
----
oc get all
----

Get all `pods`:

[source,shell]
----
oc get pods -o wide
----

This will output something similar to this:

[source,shell]
----
NAME               READY     STATUS      RESTARTS   AGE       IP          NODE                      NOMINATED NODE
nodetest-1-2g2dz   1/1       Running     0          23h       10.1.2.67   node1.jhb-94d8.internal   <none>
nodetest-1-54fw7   1/1       Running     0          3h        10.1.2.74   node1.jhb-94d8.internal   <none>
nodetest-1-6xw6g   1/1       Running     0          3h        10.1.2.75   node1.jhb-94d8.internal   <none>
nodetest-1-build   0/1       Completed   0          23h       10.1.2.65   node1.jhb-94d8.internal   <none>
----

TIP: Note the pod used to build the project is there, just inactive. +
Also note the differing IPs for the individual Pods and the NODE information.

In the Web Console, make sure you are on the btn:[Overview] page, then do the following in CLI while watching the page:

[source,shell]
----
oc delete pod nodetest-****
----
(Replace ******** with once of the running pods)

image::screenshot_deleting_pod.png[Deleting a pod]

****
IMPORTANT: *TODO*

Explain the nature of Liveness (kill/restart) and Readiness (if not ready Pod IP is removed from the round-robin HAProxy)

****

[[lifecycle-health-checks]]
==== Health Checks

In the Web Console, go to menu:Applications[Deployments > nodetest > Configuration].

Under Template, click `Add Health Checks`:

image::screenshot_add_health.png[Adding Health Checks]

TIP:
Click on the `Learn More` link or 
here: https://docs.openshift.com/container-platform/3.11/dev_guide/application_health.html[https://docs.openshift.com/container-platform/3.11/dev_guide/application_health.html]
to read more about Health probes

****
IMPORTANT: *TODO*

Explain the concepts of the readiness and health probes

****

[[lifecycle-rolling-recreate]]
==== Application Deployment Strategies

From the menu: menu:Applications[Deployment > nodetest > Configuration]

****
IMPORTANT: *TODO*

Explain rolling and recreate - explain deployment triggers (image and config)

****

In the top right corner, click the btn:[Actions > Edit] button.

Change the btn:[Strategy Type] to `Recreate` and click btn:[Save]

image::screenshot_deployment_recreate.png[Recreate]

Now go to menu:Applications[Deployments > notetest]

TIP: Note that Deployment \#1 is active.

Click the btn:[Deploy] button (top right) and the quickly go back to the menu:Overview[] page.

image::screenshot_deployment_recreate_pod_ring.png[Recreate in action]

TIP: Note that all instances is being recreate and there is zero instances available above.

Go back to menu:Applications[Deployments > notetest]

TIP: Note that Deployment \#2 is active.

Change back to Rolling Strategy: btn:[Actions > Edit] then change the
btn:[Strategy Type] to `Rolling` and click btn:[Save]

Now again click the btn:[Deploy] and quickly go back to the menu:Overview[] page.

image::screenshot_deployment_rolling_pod_ring.png[Rolling in action]

TIP: Note that the number of available pods never drops beneath the required number of replicas

Read more about deployment strategies here: https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html[https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html]

[[lifecycle-storage]]
==== Storage

[UI] Storage/Create Storage, default storage class, name=’test’, access mode RWO, size=1GB, ‘Create’
Explain the nature of PVs, how they are exported to all nodes, how the Container Runtime maps them into the Container at deployment as an additional file system
[UI] Applications/Deployments/nodetest/Configuration, scroll down, click ‘Add Storage’
Explain the nature of a PVC which locks the storage into the container
[UI] Test PV should be selected, set the mount path to something unique to the attendee (i.e. /uth), click ‘Add’
[UI] Back to Overview
Note the redeployment, explain again the trigger of config change as well as image
[UI] Click on Pod ring - scroll down, select the top Pod
[UI] Select Terminal - type id, type df -h, type ps -ef
Explain the ‘it thinks it is an OS’ concept, explain SELinux constraints around the ‘ps -ef’, note the addition of the new disk at the mount point provided at the PV creation
[UI] In the Pod terminal, do ‘cd (themountpointforthePV)’ - do ‘touch test.txt’
[UI] In the Pod terminal, do ‘ls -alZ’
Explain the selinux constraints
[oc] do ‘oc get pods -o wide’
Find two Pods on physically separate Nodes - take note of the Pod names - explain the format, name-(x)-(randomchars)
[UI] Go to Overview, click on the Pod, scroll down, choose the first named Pod, select Terminal
[UI] cd /(mountpointforpv), vi test.txt, [i], type some chars, [ESC]:wq
[UI] Go to Overview, click on the other Pod (insure it’s somewhere physically different), cd /(mountpointforpv), cat test.txt
Explain the nature of the single file in persisted storage across multiple physical nodes

[[lifecycle-config-maps]]
==== Config Maps

[UI] Resources/Config Maps - ‘Create Config Map’
Discuss the nature of config maps as environment vars
[UI] Name = ‘configmapenv’, Key = “CONFIGENV’, Value = “somevaluefortheenv”, ‘Create’
[UI] Applications/Deployments/nodetest/Environment, Environment From - select configmapenv, click ‘Add ALL Values from Config Map’, ‘Save’
[UI] Back to Overview, watch the deployment finish
[UI] Click on the Pod ring, scroll down, choose a Pod, choose Terminal, type ‘env | grep CONFIGENV’
Explain the relevance of the environment variable - not part of the deployment, applied at the container level
[UI] Resources/Config Maps - ‘Create Config Map’
Discuss the nature of config maps as an embedded overlay file (overwriting image contents)
[UI] Name = ‘configmapfile’, Key = ‘myapp.conf’, Value=’hello!’, ‘Create’
[UI] Applications/Deployments/nodetest/Configuration/Add Config Files, Source = configmapfile, Mount Path = ‘/config/app’, ‘Add’
[UI] Back to Overview, watch deployment complete
[UI] Click on Pod ring, scroll down, click on a Pod, Terminal, ‘cd /config/app’, ‘cat myapp.conf’
Explain the nature of the config map being written as a file into the container file system - external to image
Discuss the difference between configmaps and secrets

[[lifecycle-secrets]]
==== Secrets

[UI] Resources/Secrets, ‘Create Secret’, type=’Generic Secret’, secret name=”nodetestsecret”, key=”mypassword”, clear value=”mydodgypassword”, ‘Create
[UI] Click on the secret ‘nodetestsecret’, click on ‘Add to Application’, select ‘nodetest’, ‘Save’
[UI] Back to Overview, watch the deployment complete, click on the Pod ring, scroll down, select a Pod
[UI] Select ‘Terminal’, type ‘env | grep password’
Explain the encrypted nature of the secret outside of the Pods
[oc] oc describe secret nodetestsecret
[oc] oc edit secret nodetestsecret
Explain the encryption of the secret at the object level

[[lifecycle-cleanup]]
=== Clean up

[oc] oc describe bc nodetest
Point out the Label (app=nodetest), explain its relevance, explain the nature of the extensible object model
[oc] oc delete all -l “app=nodetest”
Explain the clean-up process
