[[lifecycle]]
== Simple application lifecycle

****
IMPORTANT: *TODO*

Describe the scripts within an S2I build (compile, assemble, execute)

****

[[lifecycle-s2i]]
=== Deploying an application using S2I

In the Catalog of your sandbox project:

* Filter (top tab bar) *Languages*
* Select *JavaScript*
* Choose *Node.js*

image::screenshot_catalog_filter_js.png[Node application]

You will now go though a wizard to gather all data needed for this S2I build config:

****
IMPORTANT: *TODO*

Explain the nature of the wizards

****

* Click btn:[Next] on first page of Wizard.

****
IMPORTANT: *TODO*

Explain the parameter fields as pertains to templates

****

* Select node image version *10*
* Enter name as `nodetest`
* Enter the following url as the github repo https://github.com/utherp0/ocpnode[https://github.com/utherp0/ocpnode]

Click btn:[Create], then btn:[Close] to close the wizard

image::screenshot_s2i_wizard.png[S2I Wizard]

Now go back to the menu:Overview[] page.

****
IMPORTANT: *TODO*

Explain the behaviour that is happening, the creation of the objects from the template, 
the build-config being used by the build, the build delivering the image into the registry, 
the deployment-config waiting on the image arrival, the default single Pod deployment, the creation of the route

****
[[lifecycle-running]]
=== The running application

When in the Overview page, you will see all running applications. Expand the `nodetest` application we just deployed.
You will see an overview of the running application:

* Information on the running container
* Number of pods and the status (a.k.a kbd:[Pod ring]) of the pods
* Networking information including internal port mapping and external routes
* Build history and information

image::screenshot_app_overview.png[Application Overview]

To see the application in action, click on the link in the external route.
This will open the basic node.js application:

image::screenshot_node_app.png[Node.js Application]

[[lifecycle-application-services]]
==== Application Services

Using the menu on the left go to the menu:Applications[Services] page.

image::screenshot_app_services.png[Application Services]

****
IMPORTANT: *TODO*

Explain the nature of the single service endpoint for the application - note the cluster IP address

****

NOTE: More info here: 
https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/pods_and_services.html#services[https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/pods_and_services.html#services]

Go back to the menu:Overview[] page.

[[lifecycle-application-pods]]
==== Application Pods

Click on the kbd:[Pod ring], or alternatively use the menu menu:Applications[Pods > nodetest-****]

image::screenshot_app_pod.png[Application Pod]

TIP: See the differing IP address for the Pod compared to the cluster IP

Go back to the menu:Overview[] page.

[[lifecycle-application-scaling]]
==== Application Scaling

Let's pretend that this app is suddenly getting many requests from many users (so there is a load increase on the app).
So we need to scale the application to 3 instances.

Click the menu:Up arrow[] (^) until there are 3 replicas.

image::screenshot_scaling_up.png[Application Scaling]

****
IMPORTANT: *TODO*

Explain the blue, gray, dark blue, red colour schemes for the Pod behaviors

****

Click on the kbd:[Pod ring], or alternatively use the menu menu:Applications[Deployments > nodetest > #1 (latest)].

Scroll down to where the Pods are listed:

image::screenshot_app_pods_3.png[Pod listing]

TIP: See the difference in age between the initial pod and the 2 recent scaled pods.

Select on of the recent (younger) pods.

TIP: Note the IP difference compared to the initial pod.

****
IMPORTANT: *TODO*

Explain the load-balancing of Pod IP endpoints from the singular cluster IP and how that abstracts from the Route.

****

[[lifecycle-application-route]]
==== Application Route

Using the menu on the left go to the menu:Applications[Routes] page.

image::screenshot_app_routes.png[Application Routes]

TIP: Note the mapping of the fully qualified domain name to the cluster IP via the service name

Select the nodetest link in the service column. 

image::screenshot_route_service.png[Route service]

TIP: Note that the route maps to the cluster IP

[[lifecycle-application-cli]]
==== Application from CLI

Now let's go to the console (either using `localhost` or `oconline` as explained in the <<setup-cli>> section)

Make sure you are still logged in:

[source,shell]
----
oc whoami
----

(if not, log in again as explained in the <<setup-login>> section)

Make sure we are using our sandbox project:

[source,shell]
----
oc project sandbox-userX
----

This will print: 

[source,shell,subs=attributes+]
----
Now using project "sandbox-userX" on server "{webConsoleUrl}:443".
----

You can find all `objects` that you can interact with in this namespace/project:

[source,shell]
----
oc get all
----

Get all `pods`:

[source,shell]
----
oc get pods -o wide
----

This will output something similar to this:

[source,shell]
----
NAME               READY     STATUS      RESTARTS   AGE       IP          NODE                      NOMINATED NODE
nodetest-1-2g2dz   1/1       Running     0          23h       10.1.2.67   node1.jhb-94d8.internal   <none>
nodetest-1-54fw7   1/1       Running     0          3h        10.1.2.74   node1.jhb-94d8.internal   <none>
nodetest-1-6xw6g   1/1       Running     0          3h        10.1.2.75   node1.jhb-94d8.internal   <none>
nodetest-1-build   0/1       Completed   0          23h       10.1.2.65   node1.jhb-94d8.internal   <none>
----

TIP: Note the pod used to build the project is there, just inactive. +
Also note the differing IPs for the individual Pods and the NODE information.

In the Web Console, make sure you are on the btn:[Overview] page, then do the following in CLI while watching the page:

[source,shell]
----
oc delete pod nodetest-****
----
(Replace ******** with once of the running pods)

image::screenshot_deleting_pod.png[Deleting a pod]

****
IMPORTANT: *TODO*

Explain the nature of Liveness (kill/restart) and Readiness (if not ready Pod IP is removed from the round-robin HAProxy)

****

[[lifecycle-health-checks]]
==== Health Checks

In the Web Console, go to menu:Applications[Deployments > nodetest > Configuration].

Under Template, click `Add Health Checks`:

image::screenshot_add_health.png[Adding Health Checks]

TIP:
Click on the `Learn More` link or 
here: https://docs.openshift.com/container-platform/3.11/dev_guide/application_health.html[https://docs.openshift.com/container-platform/3.11/dev_guide/application_health.html]
to read more about Health probes

****
IMPORTANT: *TODO*

Explain the concepts of the readiness and health probes

****

[[lifecycle-rolling-recreate]]
==== Application Deployment Strategies

From the menu: menu:Applications[Deployment > nodetest > Configuration]

****
IMPORTANT: *TODO*

Explain rolling and recreate - explain deployment triggers (image and config)

****

In the top right corner, click the btn:[Actions > Edit] button.

Change the btn:[Strategy Type] to `Recreate` and click btn:[Save]

image::screenshot_deployment_recreate.png[Recreate]

Now go to menu:Applications[Deployments > notetest]

TIP: Note that Deployment \#1 is active.

Click the btn:[Deploy] button (top right) and the quickly go back to the menu:Overview[] page.

image::screenshot_deployment_recreate_pod_ring.png[Recreate in action]

TIP: Note that all instances is being recreate and there is zero instances available above.

Go back to menu:Applications[Deployments > notetest]

TIP: Note that Deployment \#2 is active.

Change back to Rolling Strategy: btn:[Actions > Edit] then change the
btn:[Strategy Type] to `Rolling` and click btn:[Save]

Now again click the btn:[Deploy] and quickly go back to the menu:Overview[] page.

image::screenshot_deployment_rolling_pod_ring.png[Rolling in action]

TIP: Note that the number of available pods never drops beneath the required number of replicas

Read more about deployment strategies here: https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html[https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html]

[[lifecycle-storage]]
==== Storage

Go to menu:Storage[] page and select btn:[Create Storage]:

* *Name:* test
* *Access Mode:* RWO
* *Size:* 1 GiB 

Click btn:[Create]

****
IMPORTANT: *TODO*

Explain the nature of PVs, how they are exported to all nodes, how the Container Runtime maps them into the Container at deployment as an additional file system

****

Now we will assign this storage to our application. Go to menu:Applications[Deployments] and select `nodetest` and select the `Configuration` tab. 
Under the Volumes section, click `Add Storage`

****
IMPORTANT: *TODO*

Explain the nature of a PVC which locks the storage into the container

****

Select the `test` storage option (This is the one we just created)

In the *Mount Path* make sure that the path is unique to you, so make it `/usrX` (Where X is your assigned ID).
Click btn:[Add]

image::screenshot_assign_vol.png[Assign Volumes]

Go back to the menu:Overview[] page.

TIP: Note the redeployment, this is because above is a config change and a new image needs to be build to make this mount point available

Click on the kbd:[Pod ring] and select the first (top) pod in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
id
----

This will print the unique id for this pod, example:

[source,shell]
----
uid=1000360000 gid=0(root) groups=0(root),1000360000
----

Now type the following in the terminal:

[source,shell]
----
df -h
----

This will report information on the disk space for this pod, example:

[source,shell]
----
Filesystem                                                 Size  Used Avail Use% Mounted on
overlay                                                     50G  7.3G   43G  15% /
tmpfs                                                       32G     0   32G   0% /dev
tmpfs                                                       32G     0   32G   0% /sys/fs/cgroup
support1.fourways-3631.internal:/srv/nfs/user-vols/vol411  197G  498M  187G   1% /usr1
/dev/xvda2                                                  50G  7.3G   43G  15% /etc/hosts
shm                                                         64M     0   64M   0% /dev/shm
tmpfs                                                       32G   16K   32G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                                                       32G     0   32G   0% /proc/acpi
tmpfs                                                       32G     0   32G   0% /proc/scsi
tmpfs                                                       32G     0   32G   0% /sys/firmware
----

You will see the volume create earlier mounted under `/usrX`.

Type the following:

[source,shell]
----
ps -ef
----

****
IMPORTANT: *TODO*

Explain the ‘it thinks it is an OS’ concept, explain SELinux constraints around the ‘ps -ef’, 
note the addition of the new disk at the mount point provided at the PV creation

****

Now let's go to the volume mount point and create a file in the root:

[source,shell]
----
cd /usrX
touch test.txt
----

List the contents of the folder:

[source,shell]
----
ls -alZ
----

You will see a list of directories and files, example:

[source,shell]
----
drwxrwxrwx. root       root  system_u:object_r:nfs_t:s0                     .
drwxr-xr-x. root       root  system_u:object_r:container_file_t:s0:c9,c19   ..
-rw-r--r--. 1000360000 65534 system_u:object_r:nfs_t:s0                     test1.txt
----

****
IMPORTANT: *TODO*

Explain the selinux constraints

****

Now, in the CLI (NOT the terminal we have been using just now), do the following:

[source,shell]
----
oc get pods -o wide
----

You will see a list of all pods, example:

[source,shell]
----
NAME               READY   STATUS      RESTARTS   AGE   IP          NODE                           NOMINATED NODE
nodetest-1-build   0/1     Completed   0          1h    10.1.4.8    node1.jhb-94d8.internal   <none>
nodetest-2-5lcdq   1/1     Running     0          15m   10.1.4.12   node1.jhb-94d8.internal   <none>
nodetest-2-7dnjv   1/1     Running     0          12m   10.1.4.14   node1.jhb-94d8.internal   <none>
nodetest-2-nfnlf   1/1     Running     0          12m   10.1.4.13   node1.jhb-94d8.internal   <none>
----

****
IMPORTANT: *TODO*

Find two Pods on physically separate Nodes - take note of the Pod names - explain the format, name-(x)-(randomchars)

****
Go back to the menu:Overview[] page.

Click on the kbd:[Pod ring] and select the first (top) pod in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
cd /usrX
vi test.txt
----

Once in `vi`, press *i* to enter *insert* mode.

Now type something, example: `Hello World`.

Then press *Esc* (to exit the insert mode) and then *:wq* to write and quit vi.
You can do a `cat` to make sure the contents is saved in the file:

[source,shell]
----
cat test.txt 
Hello world
----

Now go back to the menu:Overview[] page.

Click on the kbd:[Pod ring] and select any pod except the first (top) one in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
cd /usrX
cat test.txt
Hello world
----

As you can see the file is available on all pods.

****
IMPORTANT: *TODO*

Explain the nature of the single file in persisted storage across multiple physical nodes

****

[[lifecycle-config-maps]]
==== Config Maps

Navigate to menu:Resources[Config Maps] and then click btn:[Create Config Map]

****
IMPORTANT: *TODO*

Discuss the nature of config maps as environment vars

****

Enter the following in the fields:
* *Name:* configmapenv
* *Key:* CONFIGENV
* *Value:* somevaluefortheenv

Then click btn:[Create]

image::screenshot_config_map.png[Config Map]

Navigate to menu:Applications[Deployments] select `nodetest` and then the `Environment` Tab.

In the `Environment From` section, select the `configmapenv` we just created.

Click the `Add ALL Values from Config Map or Secret` link and then btn:[Save].

Now go back to the menu:Overview[] page, and watch the deployment finish.

Click on the kbd:[Pod ring] and select the first (top) pod in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
env | grep CONFIGENV
----

You will see the key/value we just created.

****
IMPORTANT: *TODO*

Explain the relevance of the environment variable - not part of the deployment, applied at the container level

****

Now let's create another config map.
Navigate back to menu:Resources[Config Maps] and then click btn:[Create Config Map]

****
IMPORTANT: *TODO*

Discuss the nature of config maps as an embedded overlay file (overwriting image contents)

****

Enter the following in the fields:
* *Name:* configmapfile
* *Key:* myapp.conf
* *Value:* hello!

Then click btn:[Create]

Navigate to menu:Applications[Deployments] select `nodetest` and then the `Configuration` Tab.

In the `Volumes` section, select `Add Config Files`:

* *Source:* configmapfile
* *Mount Path:* /config/app

Click btn:[Add]

Now go back to the menu:Overview[] page, and watch the deployment finish.

Click on the kbd:[Pod ring] and select the first (top) pod in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
cd /config/app
cat myapp.conf
----

You will see the value we just created.

****
IMPORTANT: *TODO*

Explain the nature of the config map being written as a file into the container file system - external to image
Discuss the difference between configmaps and secrets

****

[[lifecycle-secrets]]
==== Secrets

[UI] Resources/Secrets, ‘Create Secret’, type=’Generic Secret’, secret name=”nodetestsecret”, key=”mypassword”, clear value=”mydodgypassword”, ‘Create
[UI] Click on the secret ‘nodetestsecret’, click on ‘Add to Application’, select ‘nodetest’, ‘Save’
[UI] Back to Overview, watch the deployment complete, click on the Pod ring, scroll down, select a Pod
[UI] Select ‘Terminal’, type ‘env | grep password’
Explain the encrypted nature of the secret outside of the Pods
[oc] oc describe secret nodetestsecret
[oc] oc edit secret nodetestsecret
Explain the encryption of the secret at the object level

[[lifecycle-cleanup]]
=== Clean up

[oc] oc describe bc nodetest
Point out the Label (app=nodetest), explain its relevance, explain the nature of the extensible object model
[oc] oc delete all -l “app=nodetest”
Explain the clean-up process
