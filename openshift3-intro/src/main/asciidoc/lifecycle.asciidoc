[[lifecycle]]
== Simple application lifecycle

=== Quick Introduction to the Build process
One of the most exciting features of OpenShift from a developer's perspective is the concept of the S2I, or *Source-2-Image* 
which provides a standardised way of taking a base image, containing, for example, the framework for running a node.js application, 
a source code repository, containing the code of the application that matches the framework provided in the base image, and constructing 
and delivering a composite Application image to the Registry. 

The standardised nature of the S2I approach means that, in the case where Red Hat do not provide out-of-the-box facilities for 
a framework, it is simple to create the S2I scripts and use them in exactly the same fashion as the out-of-the-box scripts.

The S2I works in effectively three phases - a compilation phase, which is OpenShift agnostic and simply, in the case of requiring it, 
compiles the source material to the binary components, or interim bytecode formats, required for the framework. In the case of Java, for example, 
this would entail executing a Maven build on the source provided in the repository. The second phase is to construct the composite
image for the Application by injecting the binary components into the appropriate filespace in the base image. The third phase is to setup 
the execution context for the Application, i.e. how it is started, for example in the case of a Java web-app running on a Tomcat base 
this would be executing the bin/startup.sh script.

A nice feature of this approach is the capability of the S2I scripts to also be executed in what is called *Binary* mode. This allows the 
script to ignore the compilation phase and be provided with the binary components required to build the Application. This is a usecase for 
developers who wish to either bring pre-compiled components for their Apps or in the situation where an organisation has an existing 
build farm for applications where the binary artefacts are tested and config controlled outside of OpenShift. This mode is called 'binary build'.

In the following example we will take advantage of the node.js builder S2I script in OpenShift.

[[lifecycle-s2i]]
=== Deploying an application using S2I

In the Catalog of your sandbox project:

* Filter (top tab bar) *Languages*
* Select *JavaScript*
* Choose *Node.js*

image::screenshot_catalog_filter_js.png[Node application]

You will now go though a wizard to gather all data needed for this S2I build config:

The UI provides Wizards based on the templates installed (which are the selections chosen from in the Catalog). These Wizards 
explain the nature of the chosen template, and then prompt the User for specific information which is required to perform the full lifecycle 
build and deployment of the target application. 

* Click btn:[Next] on first page of Wizard.

Each template has a set of parameters that are either optional or mandatory. The Wizard provides a way for the User to enter the 
information - in the case of the node.js there are only three pieces of mandatory information required to create, build and deploy the 
node.js application. The Wizards also provide additional 'advanced' options which are the optional parameters for specialising the build, for 
example if you are using a code repository that has secure access you need to provide what is known as a *Pull Secret* which will allow 
OpenShift, which is running the Build as a standalone Pod, to clone the source material from your repository. This is one of the optional parameters. 

* Select node image version *10*
* Enter name as `nodetest`
* Enter the following url as the github repo https://github.com/utherp0/ocpnode[https://github.com/utherp0/ocpnode]

Click btn:[Create], then btn:[Close] to close the wizard

image::screenshot_s2i_wizard.png[S2I Wizard]

Now go back to the menu:Overview[] page.

****
IMPORTANT: *TODO*

Explain the behaviour that is happening, the creation of the objects from the template, 
the build-config being used by the build, the build delivering the image into the registry, 
the deployment-config waiting on the image arrival, the default single Pod deployment, the creation of the route

****

[[lifecycle-running]]
=== The running application

When in the Overview page, you will see all running applications. Expand the `nodetest` application we just deployed.
You will see an overview of the running application:

* Information on the running container
* Number of pods and the status (a.k.a kbd:[Pod ring]) of the pods
* Networking information including internal port mapping and external routes
* Build history and information

image::screenshot_app_overview.png[Application Overview]

To see the application in action, click on the link in the external route.
This will open the basic node.js application:

image::screenshot_node_app.png[Node.js Application]

[[lifecycle-application-services]]
==== Application Services

Using the menu on the left go to the menu:Applications[Services] page.

image::screenshot_app_services.png[Application Services]

****
IMPORTANT: *TODO*

Explain the nature of the single service endpoint for the application - note the cluster IP address

****

NOTE: More info here: 
https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/pods_and_services.html#services[https://docs.openshift.com/container-platform/3.11/architecture/core_concepts/pods_and_services.html#services]

Go back to the menu:Overview[] page.

[[lifecycle-application-pods]]
==== Application Pods

Click on the kbd:[Pod ring], or alternatively use the menu menu:Applications[Pods > nodetest-****]

image::screenshot_app_pod.png[Application Pod]

TIP: See the differing IP address for the Pod compared to the cluster IP

Go back to the menu:Overview[] page.

[[lifecycle-application-scaling]]
==== Application Scaling

Let's pretend that this app is suddenly getting many requests from many users (so there is a load increase on the app).
So we need to scale the application to 3 instances.

Click the menu:Up arrow[] (^) until there are 3 replicas.

image::screenshot_scaling_up.png[Application Scaling]

****
IMPORTANT: *TODO*

Explain the blue, gray, dark blue, red colour schemes for the Pod behaviors

****

Click on the kbd:[Pod ring], or alternatively use the menu menu:Applications[Deployments > nodetest > #1 (latest)].

Scroll down to where the Pods are listed:

image::screenshot_app_pods_3.png[Pod listing]

TIP: See the difference in age between the initial pod and the 2 recent scaled pods.

Select on of the recent (younger) pods.

TIP: Note the IP difference compared to the initial pod.

****
IMPORTANT: *TODO*

Explain the load-balancing of Pod IP endpoints from the singular cluster IP and how that abstracts from the Route.

****

[[lifecycle-application-route]]
==== Application Route

Using the menu on the left go to the menu:Applications[Routes] page.

image::screenshot_app_routes.png[Application Routes]

TIP: Note the mapping of the fully qualified domain name to the cluster IP via the service name

Select the nodetest link in the service column. 

image::screenshot_route_service.png[Route service]

TIP: Note that the route maps to the cluster IP

[[lifecycle-application-cli]]
==== Application from CLI

Now let's go to the console (either using `localhost` or `oconline` as explained in the <<setup-cli>> section)

Make sure you are still logged in:

[source,shell]
----
oc whoami
----

(if not, log in again as explained in the <<setup-login>> section)

Make sure we are using our sandbox project:

[source,shell]
----
oc project sandbox-userX
----

This will print: 

[source,shell,subs=attributes+]
----
Now using project "sandbox-userX" on server "{webConsoleUrl}:443".
----

You can find all `objects` that you can interact with in this namespace/project:

[source,shell]
----
oc get all
----

Get all `pods`:

[source,shell]
----
oc get pods -o wide
----

This will output something similar to this:

[source,shell]
----
NAME               READY     STATUS      RESTARTS   AGE       IP          NODE                      NOMINATED NODE
nodetest-1-2g2dz   1/1       Running     0          23h       10.1.2.67   node1.jhb-94d8.internal   <none>
nodetest-1-54fw7   1/1       Running     0          3h        10.1.2.74   node1.jhb-94d8.internal   <none>
nodetest-1-6xw6g   1/1       Running     0          3h        10.1.2.75   node1.jhb-94d8.internal   <none>
nodetest-1-build   0/1       Completed   0          23h       10.1.2.65   node1.jhb-94d8.internal   <none>
----

TIP: Note the pod used to build the project is there, just inactive. +
Also note the differing IPs for the individual Pods and the NODE information.

In the Web Console, make sure you are on the btn:[Overview] page, then do the following in CLI while watching the page:

[source,shell]
----
oc delete pod nodetest-****
----
(Replace ******** with once of the running pods)

image::screenshot_deleting_pod.png[Deleting a pod]

****
IMPORTANT: *TODO*

Explain the nature of Liveness (kill/restart) and Readiness (if not ready Pod IP is removed from the round-robin HAProxy)

****

[[lifecycle-health-checks]]
==== Health Checks

In the Web Console, go to menu:Applications[Deployments > nodetest > Configuration].

Under Template, click `Add Health Checks`:

image::screenshot_add_health.png[Adding Health Checks]

TIP:
Click on the `Learn More` link or 
here: https://docs.openshift.com/container-platform/3.11/dev_guide/application_health.html[https://docs.openshift.com/container-platform/3.11/dev_guide/application_health.html]
to read more about Health probes

****
IMPORTANT: *TODO*

Explain the concepts of the readiness and health probes

****

[[lifecycle-rolling-recreate]]
==== Application Deployment Strategies

From the menu: menu:Applications[Deployment > nodetest > Configuration]

****
IMPORTANT: *TODO*

Explain rolling and recreate - explain deployment triggers (image and config)

****

In the top right corner, click the btn:[Actions > Edit] button.

Change the btn:[Strategy Type] to `Recreate` and click btn:[Save]

image::screenshot_deployment_recreate.png[Recreate]

Now go to menu:Applications[Deployments > notetest]

TIP: Note that Deployment \#1 is active.

Click the btn:[Deploy] button (top right) and the quickly go back to the menu:Overview[] page.

image::screenshot_deployment_recreate_pod_ring.png[Recreate in action]

TIP: Note that all instances is being recreate and there is zero instances available above.

Go back to menu:Applications[Deployments > notetest]

TIP: Note that Deployment \#2 is active.

Change back to Rolling Strategy: btn:[Actions > Edit] then change the
btn:[Strategy Type] to `Rolling` and click btn:[Save]

Now again click the btn:[Deploy] and quickly go back to the menu:Overview[] page.

image::screenshot_deployment_rolling_pod_ring.png[Rolling in action]

TIP: Note that the number of available pods never drops beneath the required number of replicas

Read more about deployment strategies here: https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html[https://docs.openshift.com/container-platform/3.11/dev_guide/deployments/deployment_strategies.html]

[[lifecycle-storage]]
==== Storage

Go to menu:Storage[] page and select btn:[Create Storage]:

* *Name:* test
* *Access Mode:* RWO
* *Size:* 1 GiB 

Click btn:[Create]

****
IMPORTANT: *TODO*

Explain the nature of PVs, how they are exported to all nodes, how the Container Runtime maps them into the Container at deployment as an additional file system

****

Now we will assign this storage to our application. Go to menu:Applications[Deployments] and select `nodetest` and select the `Configuration` tab. 
Under the Volumes section, click `Add Storage`

****
IMPORTANT: *TODO*

Explain the nature of a PVC which locks the storage into the container

****

Select the `test` storage option (This is the one we just created)

In the *Mount Path* make sure that the path is unique to you, so make it `/usrX` (Where X is your assigned ID).
Click btn:[Add]

image::screenshot_assign_vol.png[Assign Volumes]

Go back to the menu:Overview[] page.

TIP: Note the redeployment, this is because above is a config change and a new image needs to be build to make this mount point available

Click on the kbd:[Pod ring] and select the first (top) pod in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
id
----

This will print the unique id for this pod, example:

[source,shell]
----
uid=1000360000 gid=0(root) groups=0(root),1000360000
----

Now type the following in the terminal:

[source,shell]
----
df -h
----

This will report information on the disk space for this pod, example:

[source,shell]
----
Filesystem                                                 Size  Used Avail Use% Mounted on
overlay                                                     50G  7.3G   43G  15% /
tmpfs                                                       32G     0   32G   0% /dev
tmpfs                                                       32G     0   32G   0% /sys/fs/cgroup
support1.fourways-3631.internal:/srv/nfs/user-vols/vol411  197G  498M  187G   1% /usr1
/dev/xvda2                                                  50G  7.3G   43G  15% /etc/hosts
shm                                                         64M     0   64M   0% /dev/shm
tmpfs                                                       32G   16K   32G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                                                       32G     0   32G   0% /proc/acpi
tmpfs                                                       32G     0   32G   0% /proc/scsi
tmpfs                                                       32G     0   32G   0% /sys/firmware
----

You will see the volume create earlier mounted under `/usrX`.

Type the following:

[source,shell]
----
ps -ef
----

****
IMPORTANT: *TODO*

Explain the ‘it thinks it is an OS’ concept, explain SELinux constraints around the ‘ps -ef’, 
note the addition of the new disk at the mount point provided at the PV creation

****

Now let's go to the volume mount point and create a file in the root:

[source,shell]
----
cd /usrX
touch test.txt
----

List the contents of the folder:

[source,shell]
----
ls -alZ
----

You will see a list of directories and files, example:

[source,shell]
----
drwxrwxrwx. root       root  system_u:object_r:nfs_t:s0                     .
drwxr-xr-x. root       root  system_u:object_r:container_file_t:s0:c9,c19   ..
-rw-r--r--. 1000360000 65534 system_u:object_r:nfs_t:s0                     test1.txt
----

****
IMPORTANT: *TODO*

Explain the selinux constraints

****

Now, in the CLI (NOT the terminal we have been using just now), do the following:

[source,shell]
----
oc get pods -o wide
----

You will see a list of all pods, example:

[source,shell]
----
NAME               READY   STATUS      RESTARTS   AGE   IP          NODE                           NOMINATED NODE
nodetest-1-build   0/1     Completed   0          1h    10.1.4.8    node1.jhb-94d8.internal   <none>
nodetest-2-5lcdq   1/1     Running     0          15m   10.1.4.12   node1.jhb-94d8.internal   <none>
nodetest-2-7dnjv   1/1     Running     0          12m   10.1.4.14   node1.jhb-94d8.internal   <none>
nodetest-2-nfnlf   1/1     Running     0          12m   10.1.4.13   node1.jhb-94d8.internal   <none>
----

****
IMPORTANT: *TODO*

Find two Pods on physically separate Nodes - take note of the Pod names - explain the format, name-(x)-(randomchars)

****
Go back to the menu:Overview[] page.

Click on the kbd:[Pod ring] and select the first (top) pod in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
cd /usrX
vi test.txt
----

Once in `vi`, press *i* to enter *insert* mode.

Now type something, example: `Hello World`.

Then press *Esc* (to exit the insert mode) and then *:wq* to write and quit vi.
You can do a `cat` to make sure the contents is saved in the file:

[source,shell]
----
cat test.txt 
Hello world
----

Now go back to the menu:Overview[] page.

Click on the kbd:[Pod ring] and select any pod except the first (top) one in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
cd /usrX
cat test.txt
Hello world
----

As you can see the file is available on all pods.

****
IMPORTANT: *TODO*

Explain the nature of the single file in persisted storage across multiple physical nodes

****

[[lifecycle-config-maps]]
==== Config Maps

Navigate to menu:Resources[Config Maps] and then click btn:[Create Config Map]

****
IMPORTANT: *TODO*

Discuss the nature of config maps as environment vars

****

Enter the following in the fields:
* *Name:* configmapenv
* *Key:* CONFIGENV
* *Value:* somevaluefortheenv

Then click btn:[Create]

image::screenshot_config_map.png[Config Map]

Navigate to menu:Applications[Deployments] select `nodetest` and then the `Environment` Tab.

In the `Environment From` section, select the `configmapenv` we just created.

Click the `Add ALL Values from Config Map or Secret` link and then btn:[Save].

Now go back to the menu:Overview[] page, and watch the deployment finish.

Click on the kbd:[Pod ring] and select the first (top) pod in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
env | grep CONFIGENV
----

You will see the key/value we just created.

****
IMPORTANT: *TODO*

Explain the relevance of the environment variable - not part of the deployment, applied at the container level

****

Now let's create another config map.
Navigate back to menu:Resources[Config Maps] and then click btn:[Create Config Map]

****
IMPORTANT: *TODO*

Discuss the nature of config maps as an embedded overlay file (overwriting image contents)

****

Enter the following in the fields:
* *Name:* configmapfile
* *Key:* myapp.conf
* *Value:* hello!

Then click btn:[Create]

Navigate to menu:Applications[Deployments] select `nodetest` and then the `Configuration` Tab.

In the `Volumes` section, select `Add Config Files`:

* *Source:* configmapfile
* *Mount Path:* /config/app

Click btn:[Add]

Now go back to the menu:Overview[] page, and watch the deployment finish.

Click on the kbd:[Pod ring] and select the first (top) pod in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
cd /config/app
cat myapp.conf
----

You will see the value we just created.

****
IMPORTANT: *TODO*

Explain the nature of the config map being written as a file into the container file system - external to image
Discuss the difference between configmaps and secrets

****

[[lifecycle-secrets]]
==== Secrets

Navigate to menu:Resources[Secrets] and then click btn:[Create Secrets].
Enter the following:

* *Secret Type:* Generic Secret
* *Secret Name:* nodetestsecret
* *Key:* mypassword
* *Value:* mydodgypassword

Click btn:[Create]

image::screenshot_secret.png[Secret]

Now select the newly created secret `nodetestsecret` and then click btn:[Add to Application].

Select the `nodetest` application and click btn:[Save].

Now go back to the menu:Overview[] page, and watch the deployment finish.

Click on the kbd:[Pod ring] and select the first (top) pod in the `Pods` section.
Select the `Terminal` tab and then type the following:

[source,shell]
----
env | grep password
----

****
IMPORTANT: *TODO*

Explain the encrypted nature of the secret outside of the Pods

****

Now, in the CLI (NOT the terminal we have been using just now), do the following:

[source,shell]
----
oc describe secret nodetestsecret
----

This will show the secret, example:

[source,shell]
----
Name:         nodetestsecret
Namespace:    sandbox-user1
Labels:       <none>
Annotations:  <none>

Type:  Opaque

Data
====
mypassword:  15 bytes
----

Now look at the secret in the object:

[source,shell]
----
oc edit secret nodetestsecret
----

Here you can see the secret is encrypted:

[source,yaml]
----
apiVersion: v1
data:
  mypassword: bXlkb2RneXBhc3N3b3Jk
kind: Secret
metadata:
  creationTimestamp: "2019-07-31T05:09:01Z"
  name: nodetestsecret
  namespace: sandbox-user1
  resourceVersion: "167161"
  selfLink: /api/v1/namespaces/sandbox-user1/secrets/nodetestsecret
  uid: 4f06e44d-b351-11e9-b116-16c647cb1fdc
type: Opaque
----

****
IMPORTANT: *TODO*

Explain the encryption of the secret at the object level

****

[[lifecycle-cleanup]]
=== Clean up

Now let's clean up everything we did in the <<lifecycle>> section:

[source,shell]
----
oc describe bc nodetest
oc delete all -l "app=nodetest"
----

****
IMPORTANT: *TODO*

Point out the Label (app=nodetest), explain its relevance, explain the nature of the extensible object model
Explain the clean-up process

****